---
title: "p8105_hw5_tq2171"
author: "Tingyu Qian"
date: "2024-11-13"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
```{r}
# Load necessary libraries
library(ggplot2)

# Function to check if there are duplicate birthdays in a group
has_duplicate_birthday <- function(group_size) {
  birthdays <- sample(1:365, group_size, replace = TRUE)  # Random birthdays
  return(any(duplicated(birthdays)))                      # Check for duplicates
}

# Simulation for each group size from 2 to 50
set.seed(123)  # For reproducibility
group_sizes <- 2:50
num_simulations <- 10000
results <- data.frame(group_size = integer(), probability = numeric())

for (n in group_sizes) {
  # Run the function 10,000 times and calculate the probability of a duplicate
  shared_birthday_count <- sum(replicate(num_simulations, has_duplicate_birthday(n)))
  probability <- shared_birthday_count / num_simulations
  
  # Store results
  results <- rbind(results, data.frame(group_size = n, probability = probability))
}

# Plotting the results
ggplot(results, aes(x = group_size, y = probability)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Probability of at Least Two People Sharing a Birthday",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()

```

Initially, the probability is very low for small group sizes, as expected, since fewer people mean fewer chances for overlap in birthdays. However, the probability grows rapidly as the group size increases. By the time the group reaches about 23 people, the probability surpasses 50%, meaning there’s a greater than even chance that at least two people share a birthday. This counterintuitive result demonstrates the birthday paradox: even in relatively small groups, the probability of shared birthdays is surprisingly high. By a group size of 50, the probability is nearly 1, indicating an almost certain chance of a shared birthday.

## Problem 2

```{r}
# load labraries and set seeds
library(tidyverse)
set.seed(1)
library(broom)
library(ggplot2)
```

```{r}
# Set parameters
n <- 30     # sample size
sigma <- 5  # standard deviation
mu_values <- 0:6  # values of mu to test
alpha <- 0.05     # significance level
num_simulations <- 5000  # number of simulations per mu
```

```{r}
# Initialize storage for results
results <- data.frame(mu = numeric(), estimate = numeric(), p_value = numeric(), reject_null = logical())
```

```{r}
# Function to simulate data, perform t-test, and store results
run_simulation <- function(mu) {
  estimates <- numeric(num_simulations)
  p_values <- numeric(num_simulations)
  reject_null <- logical(num_simulations)
  
  for (i in 1:num_simulations) {
    # Generate sample data
    data <- rnorm(n, mean = mu, sd = sigma)
    
    # Perform t-test
    test <- t.test(data, mu = 0)
    tidy_test <- tidy(test)
    
    # Store results
    estimates[i] <- tidy_test$estimate
    p_values[i] <- tidy_test$p.value
    reject_null[i] <- tidy_test$p.value < alpha
  }
  
  data.frame(mu = mu, estimate = estimates, p_value = p_values, reject_null = reject_null)
}

```

```{r}
# Run simulations for each value of mu
for (mu in mu_values) {
  results <- rbind(results, run_simulation(mu))
}

# Calculate power and average estimate for each mu
power_analysis <- results %>%
  group_by(mu) %>%
  summarise(
    power = mean(reject_null),
    avg_estimate_all = mean(estimate),
    avg_estimate_rejected = mean(estimate[reject_null])
  )

```

```{r}
# Plot 1: Power vs. Effect Size (mu)
ggplot(power_analysis, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(title = "Power vs Effect Size", x = "True Value of Mu", y = "Power (Proportion of Null Rejected)")
```
The plot shows a clear positive association between effect size (μ) and the power of the test (the probability of rejecting the null hypothesis when it is false). As the true value of μ increases, the power also increases, reaching nearly 100% when μ is around 4 or higher.

This trend indicates that larger effect sizes make it easier for the test to detect a difference from the null hypothesis (μ=0), thus increasing the likelihood of rejecting the null hypothesis correctly. When the effect size is small (close to 0), the power is low, meaning there is a higher chance of failing to detect the true effect. As the effect size grows, the test becomes more sensitive, and the power approaches 1.

```{r}
# Plot 2 average estimate of mu across all samples and only those where the null was rejected
ggplot(power_analysis, aes(x = mu)) +
  geom_line(aes(y = avg_estimate_all, color = "All Samples")) +
  geom_point(aes(y = avg_estimate_all, color = "All Samples")) +
  geom_line(aes(y = avg_estimate_rejected, color = "Null Rejected Only")) +
  geom_point(aes(y = avg_estimate_rejected, color = "Null Rejected Only")) +
  labs(
    title = "Average Estimate of Mu vs True Value of Mu",
    x = "True Value of Mu",
    y = "Average Estimate of Mu"
  ) +
  scale_color_manual(name = "Samples", values = c("All Samples" = "blue", "Null Rejected Only" = "red")) +
  theme_minimal()
```

In the plot, the sample average of \(\hat{\mu}\) across tests for which the null hypothesis was rejected (shown in red) tends to be higher than the true value of \(\mu\), especially at lower values of \(\mu\). This discrepancy occurs due to selection bias: by only considering samples where the null hypothesis was rejected, we are effectively selecting cases with larger deviations from the null. 

When \(\mu\) is small (close to zero), the test requires larger sample means to reject the null hypothesis, leading to an overestimation of \(\mu\) in those cases. As \(\mu\) increases, the bias decreases because the true effect size is large enough that the sample mean is more likely to reflect the true \(\mu\). Therefore, the average estimate across tests where the null is rejected is not consistently equal to the true \(\mu\) due to this selection effect, particularly for smaller values of \(\mu\).

## Problem 3
```{r}
# Load the data
data <- read.csv("./data/homicide-data.csv")
```

There are `r nrow(data)` observations in the dataset 'homicide', which means there are `r nrow(data)` rows and `r ncol(data)` variables in the dataset. These 12 variables are `r names(data)`. The Washington Post collected data on more than 52,000 criminal homicides over the past decade in 50 of the largest American cities. The uid variable is a unique identifier assigned to each case. Reported_date records the date the homicide was reported, formatted as YYYYMMDD. The victim_last and victim_first variables capture the victim’s last and first names, respectively, providing basic identifying information. The victim_race variable indicates the race of the victim, which could be used to explore demographic patterns in homicide cases. Similarly, victim_age records the age of the victim, and victim_sex indicates the victim’s gender, allowing for demographic analysis by age and sex. The city and state variables identify the location of each homicide by specifying the city and state where it occurred, while lat (latitude) and lon (longitude) provide more precise geographical coordinates. Finally, the disposition variable indicates the outcome of each case, specifying whether it was "Closed without arrest," "Open/No arrest," or "Closed by arrest."

```{r}
# Create city_state variable
data <- data %>%
  mutate(city_state = paste(city, state, sep = ", "))
```

```{r}
# Summarize total homicides and unsolved homicides (assume disposition column indicates solved/unsolved)
city_summary <- data %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
city_summary
```

```{r}
# Filter data for Baltimore, MD
baltimore_data <- data %>%
  filter(city == "Baltimore" & state == "MD")

# Summarize total and unsolved homicides in Baltimore
baltimore_summary <- baltimore_data %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

# Perform proportion test
baltimore_prop_test <- prop.test(
  baltimore_summary$unsolved_homicides,
  baltimore_summary$total_homicides
)

# Use broom::tidy to convert the test output to a tidy dataframe
baltimore_results <- tidy(baltimore_prop_test)

# Extract the estimated proportion and confidence intervals
baltimore_estimate <- baltimore_results %>%
  select(estimate, conf.low, conf.high)

# Display the results
baltimore_estimate
```

```{r}
# Function to perform prop.test and return a tidy dataframe
run_prop_test <- function(total, unsolved) {
  test_result <- prop.test(unsolved, total)
  tidy(test_result)
}

# Apply prop.test to each city and extract estimates and confidence intervals
results <- city_summary %>%
  mutate(
    test_result = map2(total_homicides, unsolved_homicides, run_prop_test)
  ) %>%
  unnest(test_result) %>%
  select(city_state, estimate, conf.low, conf.high)

# Display the final tidy dataframe with estimates and confidence intervals
results
```

```{r}
# Plotting the estimates and confidence intervals for each city
ggplot(results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  coord_flip() +
  theme_minimal()
```






